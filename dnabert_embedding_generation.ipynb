{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertConfig, AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "path = config['working_dir']\n",
    "\n",
    "output_dir = os.path.join(path,'output_bert_no_dim_new_new2') # Directory di output\n",
    "print('output_dir\\n',output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)  # Crea la directory di output se non esiste\n",
    "\n",
    "# Configurazione del logger\n",
    "logging.basicConfig(filename=os.path.join(output_dir, 'dna_embedding_generation.log'), \n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Percorsi dei file con i dati\n",
    "nodes_file_path = config['nodes_file_path']  # Percorso del file TSV di input\n",
    "print('nodes_file_path\\n',nodes_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento del modello e tokenizer\n",
    "config = BertConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\")\n",
    "dnabert_model = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, config=config)\n",
    "dnabert_tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "logger.info(\"Modelli e tokenizer caricati con successo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per calcolare l'embedding\n",
    "def calculate_embedding(seq):\n",
    "    if pd.isna(seq):  # Se il testo è NaN, restituiamo None\n",
    "        return None\n",
    "    try:\n",
    "        inputs = dnabert_tokenizer(seq, return_tensors='pt')[\"input_ids\"]\n",
    "        hidden_states = dnabert_model(inputs)[0]  # [1, sequence_length, 768]\n",
    "\n",
    "        # embedding with mean pooling\n",
    "        embedding_mean = torch.mean(hidden_states[0], dim=0)\n",
    "        \n",
    "        return embedding_mean.detach().cpu().numpy().tolist()  # Converti in lista\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Errore durante il calcolo dell'embedding per il testo: len {str(len(seq))}  {seq}. Errore: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per calcolare l'embedding finale\n",
    "def calculate_final_embedding(sequences):\n",
    "    embeddings_list = []  # Lista per salvare gli embedding delle sequenze\n",
    "\n",
    "    for seq in sequences:\n",
    "        if not seq:  # Se la sequenza è vuota, salta\n",
    "            logger.warning(\"Trovata sequenza vuota. Saltata.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Calcola l'embedding per la sequenza corrente\n",
    "            embedding = calculate_embedding(seq)\n",
    "            if embedding is not None:\n",
    "                embeddings_list.append(embedding)  # Aggiungi l'embedding alla lista\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Errore durante il calcolo dell'embedding per la sequenza: {seq}. Errore: {e}\")\n",
    "\n",
    "    if not embeddings_list:  # Se non ci sono embedding validi\n",
    "        logger.error(\"Nessun embedding valido calcolato.\")\n",
    "        return None\n",
    "\n",
    "    # Calcola la media componente per componente\n",
    "    final_embedding = np.mean(embeddings_list, axis=0).tolist()  # Converti in lista\n",
    "    return final_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lettura del file TSV\n",
    "df = pd.read_csv(nodes_file_path, sep='\\t')\n",
    "len0 = df.groupby('type')['name'].nunique()\n",
    "print(\"\\nCount of names by type :\",len0)\n",
    "\n",
    "# Filtraggio del dataframe per i tipi \"Gene\" e \"miRNA\"\n",
    "df_dna = df[df[\"type\"].isin([\"Gene\", \"miRNA\"])]\n",
    "logger.info(f\"DNA shape iniziale: {df_dna.shape}\")\n",
    "\n",
    "# Sostituzione di 'U' con 'T' nelle sequenze\n",
    "df_dna.loc[:, 'Sequence'] = df_dna['Sequence'].str.replace('U', 'T')\n",
    "\n",
    "# Calcolo della lunghezza delle sequenze\n",
    "df_dna.loc[:, 'len_seq'] = df_dna['Sequence'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "\n",
    "chunk_size = 512  # Dimensione del chunk\n",
    "df_dna.loc[:, 'seq_list'] = df_dna['Sequence'].apply(lambda seq: [seq[i:i + chunk_size] for i in range(0, len(seq), chunk_size)] if isinstance(seq, str) else [])\n",
    "df_dna.loc[:, 'seq_list_len'] = df_dna['seq_list'].apply(len)\n",
    "\n",
    "print(df_dna.shape)\n",
    "df_dna.head()\n",
    "\n",
    "# SettingWithCopyWarning: \n",
    "# A value is trying to be set on a copy of a slice from a DataFrame.\n",
    "# Try using .loc[row_indexer,col_indexer] = value instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len0 = df_dna[df_dna['len_seq']==0].groupby('type')['name'].nunique()\n",
    "print(\"\\nCount of names with seq_len=0 by type :\",len0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializzazione del file di output\n",
    "output_file_path = os.path.join(output_dir, \"dna_embeddings.tsv\")\n",
    "output_file_path_discarded = os.path.join(output_dir, \"dna_embeddings_discarded.tsv\")\n",
    "\n",
    "# Controlla se il file di output esiste già e leggi il numero di righe elaborate\n",
    "if os.path.exists(output_file_path):\n",
    "    df_existing = pd.read_csv(output_file_path, sep='\\t')\n",
    "    processed_count = df_existing.shape[0]\n",
    "    logger.info(f\"Ripresa dell'elaborazione dal punto {processed_count}.\")\n",
    "else:\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        f.write(\"name\\ttype\\tlen_seq\\tembedding\\n\")\n",
    "    processed_count = 0\n",
    "\n",
    "    \n",
    "batch_size = 5  # Riduci il batch size per ridurre l'uso di memoria\n",
    "embeddings_batch = []\n",
    "\n",
    "# Iterazione sulle sequenze e calcolo degli embedding\n",
    "for index, row in df_dna.iloc[processed_count:].iterrows():\n",
    "    sequence_name = row['name']\n",
    "    sequence_type = row['type']\n",
    "    sequence_list = row['seq_list']\n",
    "    sequence_len = row['len_seq']\n",
    "        \n",
    "    embedding = calculate_final_embedding(sequence_list)\n",
    "\n",
    "    if embedding is None:\n",
    "        logger.warning(f\"Embedding non calcolato per la sequenza {sequence_name} di lunghezza {sequence_len}.\")\n",
    "        \n",
    "        # output scartati \n",
    "        if not os.path.exists(output_file_path_discarded):\n",
    "            with open(output_file_path_discarded, 'w') as f:\n",
    "                f.write(\"name\\ttype\\tlen_seq\\tembedding\\n\")\n",
    "        \n",
    "        with open(output_file_path_discarded, 'a') as f:\n",
    "            f.write(f\"{sequence_name}\\t{sequence_type}\\t{sequence_len}\\n\")\n",
    "            \n",
    "    \n",
    "    embeddings_batch.append((sequence_name, sequence_type, sequence_len, embedding))\n",
    "    processed_count += 1\n",
    "\n",
    "    print(str(processed_count)+\"/\"+str(df_dna.shape[0])+\" name=\"+sequence_name)\n",
    "    logger.info(str(processed_count)+\"/\"+str(df_dna.shape[0])+\" name=\"+sequence_name)\n",
    "\n",
    "    # Scrive ogni batch_size \n",
    "    if len(embeddings_batch) >= batch_size:\n",
    "        with open(output_file_path, 'a') as f:\n",
    "            for sequence_name, sequence_type, sequence_len, embedding in embeddings_batch:\n",
    "                f.write(f\"{sequence_name}\\t{sequence_type}\\t{sequence_len}\\t{embedding}\\n\")\n",
    "        embeddings_batch = []\n",
    "        # torch.cuda.empty_cache()  # Libera la memoria GPU\n",
    "        logger.info(f\"Scritti {batch_size} embedding nel file. Totale elaborati: {processed_count}\")\n",
    "\n",
    "# Scrive eventuali rimanenti\n",
    "if embeddings_batch:\n",
    "    with open(output_file_path, 'a') as f:\n",
    "        for sequence_name, sequence_type, sequence_len, embedding in embeddings_batch:\n",
    "            f.write(f\"{sequence_name}\\t{sequence_type}\\t{sequence_len}\\t{embedding}\\n\")\n",
    "    logger.info(f\"Scritti {len(embeddings_batch)} embedding rimanenti nel file. Totale elaborati: {processed_count}\")\n",
    "\n",
    "logger.info(\"Elaborazione completata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df_output = pd.read_csv(os.path.join(output_dir, \"dna_embeddings.tsv\"), sep='\\t')\n",
    "df_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_emb = df_output[df_output['embedding'].isnull()].groupby('type')['name'].nunique()\n",
    "print(\"\\nCount of names with no embedding by type (considera solo Gene e miRna):\", no_emb)\n",
    "\n",
    "len0 = df_output[df_output['len_seq']==0].groupby('type')['name'].nunique()\n",
    "print(\"\\nCount of names with seq_len=0 by type (considera solo Gene e miRna):\",len0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_disc = pd.read_csv(os.path.join(output_dir, \"dna_embeddings_discarded.tsv\"), sep='\\t')\n",
    "# df_disc[df_disc['len_seq']!=0].groupby('type')['name'].nunique()\n",
    "# df_disc.shape\n",
    "# # Identificare i valori duplicati\n",
    "# duplicated_values = df_disc[df_disc.duplicated(subset=['name', 'type', 'len_seq'], keep=False)]\n",
    "# duplicated_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
