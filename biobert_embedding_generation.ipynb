{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "\n",
    "\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "path = config['working_dir']\n",
    "\n",
    "output_dir = os.path.join(path,'output_bert_no_dim_new_new3') # Directory di output\n",
    "print('output_dir: ',output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)  # Crea la directory di output se non esiste\n",
    "\n",
    "# Configurazione del logger\n",
    "logging.basicConfig(filename=os.path.join(output_dir, 'txt_embedding_generation.log'), \n",
    "                    level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = config['nodes_file_path']  # Percorso del file TSV di input\n",
    "print('nodes_file_path:',file_path)\n",
    "\n",
    "# Carica solo le colonne necessarie dal file TSV\n",
    "logger.info(\"Caricamento del file TSV...\")\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", usecols=[\"name\", \"type\", \"Description\", \"Sequence\"])\n",
    "logger.info(f\"File TSV caricato. Numero di righe: {len(df)}\")\n",
    "\n",
    "# Carica i modelli e i tokenizer\n",
    "logger.info(\"Caricamento dei modelli e tokenizer...\")\n",
    "biobert_tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\", trust_remote_code=True)\n",
    "biobert_model = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\", trust_remote_code=True)\n",
    "\n",
    "# Crea la colonna 'embedding' se non esiste\n",
    "df[\"embedding\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Funzione per ottenere embedding da BioBERT\n",
    "def get_text_embedding(text):\n",
    "    if pd.isna(text):  # Se il testo è NaN, restituiamo None\n",
    "        return None\n",
    "    try:\n",
    "        inputs = biobert_tokenizer(text, return_tensors='pt')['input_ids']\n",
    "        hidden_states = biobert_model(inputs)[0]  # [1, sequence_length, 768]\n",
    "\n",
    "        embedding_mean = torch.mean(hidden_states[0], dim=0)  # Mean pooling\n",
    "\n",
    "        return embedding_mean.detach().cpu().numpy().tolist()  # Converti in lista\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Errore durante il calcolo dell'embedding per il testo: len {len(text)}  {text}. \\nErrore: {e}\")\n",
    "        return None\n",
    "\n",
    "# Funzione per salvare i dati in batch\n",
    "def save_batch(df, file_path):\n",
    "    if not os.path.exists(os.path.dirname(file_path)):\n",
    "        os.makedirs(os.path.dirname(file_path))\n",
    "    \n",
    "    # Se il file esiste già, caricalo e aggiungi i nuovi dati\n",
    "    if os.path.exists(file_path):\n",
    "        existing_df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "        df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    \n",
    "    # Salva solo le colonne 'name' e 'embedding'\n",
    "    df[[\"name\", 'type', \"embedding\"]].to_csv(file_path, sep=\"\\t\", index=False)\n",
    "    os.sync()  # Sincronizza i buffer del sistema operativo\n",
    "    logger.info(f\"Batch salvato in {file_path}\")\n",
    "\n",
    "# Funzione per caricare gli indici già processati\n",
    "def load_processed_indices(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        return set(pd.read_csv(file_path, sep=\"\\t\").index.tolist())\n",
    "    return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica se tutti gli embedding di testo sono già stati calcolati\n",
    "text_output_path = os.path.join(output_dir, \"text_embeddings.tsv\")\n",
    "text_output_filled_path = os.path.join(output_dir, \"text_embeddings_filled.tsv\")\n",
    "processed_indices_path = os.path.join(output_dir, \"processed_text_indices.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1. Calcola gli embedding per BioBERT (Phenotype, Disease, Genomic feature) ----\n",
    "logger.info(\"Inizio calcolo embedding per BioBERT...\")\n",
    "df_text = df[df[\"type\"].isin([\"Phenotype\", \"Disease\", \"Genomic feature\"])].copy()\n",
    "logger.info(f\"Text shape {df_text.shape}\")\n",
    "df_text\n",
    "\n",
    "processed_indices = load_processed_indices(processed_indices_path)\n",
    "\n",
    "if os.path.exists(text_output_path):\n",
    "    existing_df = pd.read_csv(text_output_path, sep=\"\\t\")\n",
    "    if len(existing_df) == len(df_text):  # Tutti gli embedding sono già stati calcolati\n",
    "        logger.info(\"Tutti gli embedding di testo sono già stati calcolati. Saltando il calcolo.\")\n",
    "        df_text = existing_df\n",
    "    else:\n",
    "        # Calcola gli embedding mancanti\n",
    "        logger.info(\"Calcolo degli embedding di testo mancanti...\")\n",
    "        for i, row in df_text.iterrows():\n",
    "            if i in processed_indices:  # Salta gli embedding già calcolati\n",
    "                continue\n",
    "            print(row[\"Description\"])\n",
    "            embedding = get_text_embedding(row[\"Description\"])\n",
    "            df_text.at[i, \"embedding\"] = embedding  # Assegna l'embedding alla riga\n",
    "            \n",
    "            # Salva ogni 10 embedding\n",
    "            if (i + 1) % 10 == 0:\n",
    "                save_batch(df_text.iloc[i-9:i+1], text_output_path)\n",
    "                with open(processed_indices_path, \"a\") as f:\n",
    "                    f.write(\"\\n\".join(map(str, range(i-9, i+1))) + \"\\n\")\n",
    "                logger.info(f\"Salvati {i+1} embedding di testo\")\n",
    "\n",
    "        # Salva gli ultimi embedding rimanenti (se ce ne sono)\n",
    "        remaining_indices = range(len(df_text) - (len(df_text) % 10), len(df_text))\n",
    "        if len(remaining_indices) > 0:\n",
    "            save_batch(df_text.iloc[remaining_indices], text_output_path)\n",
    "            with open(processed_indices_path, \"a\") as f:\n",
    "                f.write(\"\\n\".join(map(str, remaining_indices)) + \"\\n\")\n",
    "            logger.info(f\"Salvati gli ultimi {len(remaining_indices)} embedding di testo\")\n",
    "else:\n",
    "    # Calcola tutti gli embedding di testo\n",
    "    logger.info(\"Calcolo di tutti gli embedding di testo...\")\n",
    "    for i, row in df_text.iterrows():\n",
    "        embedding = get_text_embedding(row[\"Description\"])\n",
    "        df_text.at[i, \"embedding\"] = embedding  # Assegna l'embedding alla riga\n",
    "        \n",
    "        # Salva ogni 10 embedding\n",
    "        if (i + 1) % 10 == 0:\n",
    "            save_batch(df_text.iloc[i-9:i+1], text_output_path)\n",
    "            with open(processed_indices_path, \"a\") as f:\n",
    "                f.write(\"\\n\".join(map(str, range(i-9, i+1))) + \"\\n\")\n",
    "            logger.info(f\"Salvati {i+1} embedding di testo\")\n",
    "\n",
    "    # Salva gli ultimi embedding rimanenti (se ce ne sono)\n",
    "    remaining_indices = range(len(df_text) - (len(df_text) % 10), len(df_text))\n",
    "    if len(remaining_indices) > 0:\n",
    "        save_batch(df_text.iloc[remaining_indices], text_output_path)\n",
    "        with open(processed_indices_path, \"a\") as f:\n",
    "            f.write(\"\\n\".join(map(str, remaining_indices)) + \"\\n\")\n",
    "        logger.info(f\"Salvati gli ultimi {len(remaining_indices)} embedding di testo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [name, type, embedding]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- 3. Sostituisci i valori nulli con la media degli embedding del tipo corrispondente ----\n",
    "logger.info(\"Sostituzione dei valori nulli con la media degli embedding...\")\n",
    "\n",
    "\n",
    "def replace_null_embeddings_with_type_mean(df):\n",
    "    \"\"\"\n",
    "    Sostituisce gli embedding nulli con la media degli embedding non nulli per tipo.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame contenente le colonne 'name', 'type', 'len_seq', 'embedding'\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con gli embedding nulli sostituiti\n",
    "    \"\"\"\n",
    "    # Converti la colonna embedding da stringa a lista di float (se necessario)\n",
    "    if isinstance(df['embedding'].iloc[0], str):\n",
    "        df['embedding'] = df['embedding'].apply(lambda x: eval(x) if pd.notna(x) else np.nan)\n",
    "    \n",
    "    # Converti le liste in array numpy\n",
    "    df['embedding'] = df['embedding'].apply(lambda x: np.array(x) if isinstance(x, list) else x)\n",
    "    \n",
    "    # Filtra i record con embedding non nulli\n",
    "    non_null_mask = df['embedding'].apply(lambda x: x is not np.nan if isinstance(x, np.ndarray) else pd.notna(x))\n",
    "    non_null_embeddings = df[non_null_mask]\n",
    "    \n",
    "    # Calcola la media degli embedding per ogni tipo\n",
    "    type_mean_embeddings = non_null_embeddings.groupby('type')['embedding'].apply(\n",
    "        lambda x: np.mean(np.stack(x.values), axis=0)\n",
    "    ).to_dict()\n",
    "    \n",
    "    # Sostituisci gli embedding nulli con la media del loro tipo\n",
    "    def fill_na_embedding(row):\n",
    "        if isinstance(row['embedding'], np.ndarray):\n",
    "            return row['embedding']\n",
    "        elif pd.isna(row['embedding']):\n",
    "            return type_mean_embeddings.get(row['type'], np.nan)\n",
    "        return row['embedding']\n",
    "    \n",
    "    df['embedding'] = df.apply(fill_na_embedding, axis=1)\n",
    "\n",
    "    def to_list(embedding):\n",
    "        if isinstance(embedding, str):\n",
    "            embedding = np.array(eval(embedding))\n",
    "        return embedding.tolist()\n",
    "    \n",
    "    # Converte gli array numpy in liste per una corretta scrittura su file come string\n",
    "    df['embedding'] = df['embedding'].apply(to_list)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_text = pd.read_csv(text_output_path, sep=\"\\t\")\n",
    "\n",
    "df_text_filled = replace_null_embeddings_with_type_mean(df_text)\n",
    "print(df_text_filled[df_text_filled['embedding'].isna()])  # Dovrebbe essere vuoto a meno che non ci siano tipi senza esempi validi\n",
    "\n",
    "# ---- 4. Salva i risultati finali ----\n",
    "if os.path.exists(text_output_filled_path):\n",
    "    logger.info(\"Esiste già un file. Saltando il calcolo.\")\n",
    "    pass \n",
    "else:\n",
    "    try:\n",
    "        logger.info(\"Salvataggio dei file finali...\")\n",
    "        save_batch(df_text_filled, text_output_filled_path)\n",
    "        logger.info(f\"File finali salvati: {text_output_filled_path} \")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Errore durante il salvataggio dei file finali: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13482245802879333"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.11967191100120544+0.18104693293571472+0.16617423295974731+0.07239675521850586)/4\n",
    "# 0.13482245802879333"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
